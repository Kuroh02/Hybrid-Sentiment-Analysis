{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2lRDcPV59EN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn import metrics\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Train.csv')  # Replace with your dataset"
      ],
      "metadata": {
        "id": "tw_IikNd7wlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing for Logistic Regression\n",
        "vectorizer = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None, use_idf=True, norm='l2', smooth_idf=True)\n",
        "data = data[['text', 'label']].dropna()\n",
        "data['label'] = data['label'].astype(int)\n",
        "\n",
        "X_tfidf = vectorizer.fit_transform(data['text'])\n",
        "y = data['label'].values\n",
        "\n",
        "# Train-Test Split for Logistic Regression\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression Model\n",
        "clf = LogisticRegressionCV(cv=6, scoring='accuracy', random_state=0, n_jobs=-1, verbose=3, max_iter=500)\n",
        "clf.fit(X_train_lr, y_train_lr)"
      ],
      "metadata": {
        "id": "J0JVfo_b718I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Logistic Regression Model\n",
        "y_pred_lr = clf.predict(X_test_lr)\n",
        "print(\"Logistic Regression Accuracy:\", metrics.accuracy_score(y_test_lr, y_pred_lr))\n",
        "\n",
        "# Save Logistic Regression Model\n",
        "pickle.dump(clf, open('clf.pkl', 'wb'))\n",
        "pickle.dump(vectorizer, open('tfidf.pkl', 'wb'))\n",
        "\n",
        "# Tokenization and Padding for LSTM\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['text'])\n",
        "sequences = tokenizer.texts_to_sequences(data['text'])\n",
        "max_sequence_length = 100\n",
        "X_lstm = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Train-Test Split for LSTM\n",
        "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_lstm, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "InxBI6Kj8BrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LSTM Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_sequence_length),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(len(set(y)), activation='softmax')  # Adjust output units based on the number of labels\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "EycxGkA28Hui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LSTM Model\n",
        "model.fit(X_train_lstm, y_train_lstm, validation_data=(X_test_lstm, y_test_lstm), epochs=5, batch_size=32)\n",
        "\n",
        "# Save LSTM Model and Tokenizer\n",
        "model.save('lstm_emotion_model.h5')\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)"
      ],
      "metadata": {
        "id": "ekUEambb8PEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction Function for Logistic Regression\n",
        "def predict_with_lr(comment):\n",
        "    preprocessed_comment = [comment]  # Input comment as a list\n",
        "    vectorized_comment = vectorizer.transform(preprocessed_comment)\n",
        "    prediction = clf.predict(vectorized_comment)\n",
        "    return \"Positive\" if prediction[0] == 1 else \"Negative\"\n",
        "\n",
        "# Prediction Function for LSTM\n",
        "def predict_with_lstm(comment):\n",
        "    processed_comment = pad_sequences(tokenizer.texts_to_sequences([comment]), maxlen=max_sequence_length)\n",
        "    prediction = model.predict(processed_comment)\n",
        "    emotion_index = np.argmax(prediction)\n",
        "    confidence = prediction[0][emotion_index]\n",
        "    return emotion_index, confidence"
      ],
      "metadata": {
        "id": "pDujTt3C8Uaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "if _name_ == \"_main_\":\n",
        "    test_comment = \"I love this product!\"\n",
        "\n",
        "    # Logistic Regression Prediction\n",
        "    lr_result = predict_with_lr(test_comment)\n",
        "    print(f\"Logistic Regression Prediction: {lr_result}\")\n",
        "\n",
        "    # LSTM Prediction\n",
        "    lstm_result, lstm_confidence = predict_with_lstm(test_comment)\n",
        "    print(f\"LSTM Prediction: Emotion Index {lstm_result}, Confidence {lstm_confidence}\")"
      ],
      "metadata": {
        "id": "GpZCMWC08ZNx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}